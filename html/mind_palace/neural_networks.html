<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/print.css" media="print">

    <title>Mind Palace</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Ironstein's Mind Palace</h1>
        <h2>Neural Networks</h2>
      </div>
    </header>

    <div class="container">
      <section id="main_content">

      <h3>#1.Neural Networks And Deep Learning</h3>
        <p>This Website is basically a book, regarding to <a href = "http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and deeplearning</a>, writtenby Michael Nielsen, and consists of a lot of basic material, explainedin excruciating detail and clarity. Must be read, because it will giveyou a feel for the entire subject, as well as a roadmap for studyingthe subject.</p>

        <p>1) Read only the first three chapters as they contain all the working knowledge required to understand the basics and the applications of neural networks. The next two chapters are a bit technical, and should be read after covering the contents of the first three chapters ingreater detail</p>

        <p>2) Write down working programs for Chapter 1 and chapter 2, as these  two chapters form the crux of the subject, and should be understood completely.</p>

        <p>3) Understanding the Contents of Chapter 1 and 2 is essential, andshould be understood with immense clarity.</p>

        <p>4) Go through the programs in chapter 3 loosely, since this chapterpoints out various downfalls of the previous code, suggests further optimisation algorithms and readings. The contents of this chapter is important from a subjective point of view, since it will provide a roadmap for future study, but the programs should be kept for tackling later.</p>

      <h3>#2.Cauchy-Schwarz Inequality</h3>
        <p>While going through the chapter 1 of the book Neural Networks and deep learning by Michael Nielsen (link provided in the previous commit), there is a portion where Cauchy-Schwarz inequality is used to prove that using gradient descent algorithm will always decrease the value of the cost function. Here are the links which i referred to for understanding the Cauchy-Schwarz inequality.</p>

        <p>1)<a href = "https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">wikipedia</a>(this link explains the basic form and gives a short prooffor the Cauchy-Schwarz inequality)<br>
        2)<a href = "https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/dot_cross_products/v/proof-of-the-cauchy-schwarz-inequality">Khan academy</a>(this video gives a complete proof of the Cauchy-Schwarz inequality, and explains the use of this theorem)

        <p>After referring to these two links (that is if you aren’t alreadyfamiliar with it), you should try to solve the exercise given in the book, to prove that the gradient descent algorithm always reduces the value of the quadratic cost function by using the Cauchy-Schwarz inequality</p>

      <h3>#3.Neural Network and Learning Machines</h3>
        <p>I have started studying the subject of neural networks, after going through the book mentioned in the earlier commit, by referring the book - <a href = "http://www.mif.vu.lt/~valdas/DNT/Literatura/Haykin09/Haykin09.pdf">Neural networks and machine Learning (3rd Edition) by Simon Haykin</a> (Though I really advice you that you get a hard copy because you will have to spend a lot of time (and I mean A LOT) with this book)</p>

        <p>This book consists of a lot of subject matter, and I am eventually planning to study this book along with the MIT’s machine learning course 6.867, which covers somewhat similar subject matter (but for now, for the basics, I’ll just stick with Simon Haykin).</p>

        <p>This book is a little tedious, and requires a lot of understanding andsearching a lot of places for complementary material, since a lot of things are considered to be prerequisites and thus only mentioned or explained vaguely.</p>

        <p>So, I will keep posting links, as a supplement material for the portions of the book that I found difficult to understand personally.</p>

      <h3>#4.Demonstration of Perceptron Convergence Theorem</h3>
        <p>This <a href = "https://www.youtube.com/watch?v=vGwemZhPlsA">youtube video</a> demonstrates the working of basic Perceptron Network, implementing the Perceptron convergence algorithm, to classify a linearly separable dataset into two classes.</p>

        <p>After reading the perceptron convergence theorem in the book Neural Networks and Learning Machines - Simon Haykin, you should clearly understand what is going on in this video, and if not, you should (as I did) watch this video at a slower speed and try to understand what exactly is going on.</p>

      <h3>#5.Bayes Theorem</h3>
        <p>In the book Neural Networks and Learning Machines (3rd Edition), Pg:55, there is a comparison between a Perceptron classifier and the Bayes Classifier. Not knowing what the Bayes’ classifier was, I tried to look it up online and stumbled upon the Bayes’ theorem.The Bayes Classifier is based on the famous Bayes’ Theorem, put forward by Thomas Bayes. The Bayes’ Theorem intrigued some of my curiosity and hence I started to search for resources that would explain it the best to me.</p>

        <p>The links below give some beautiful explanation (both intuitive and technical) of the Bayes’ theorem, and its implications.
        <br><a href = "http://www.yudkowsky.net/rational/bayes">intuitive explanation</a>
        <br><a href = "http://www.yudkowsky.net/rational/technical">technical explaination</a>
        </p>

        <p>Even though the discussion given in the above links is mostly off topic and has little to do with the Bayes’ classifier, I personally found it very interesting and recommend reading it.</p>

        <p>Also, the website (<a href = "http://www.yudkowsky.net">http://www.yudkowsky.net</a>) contains a lot of other blogs, which are also very interesting and one should give it a try.</p>

      <h3>#6.Interesting Artificial Intelligence Websites</h3>
        <p>In my earlier commit, I mentioned the site (<a href = "http://www.yudkowsky.net">http://www.yudkowsky.net</a>). This site is basically a blog by Eliezer S. Yudokowsky, a research fellow at the Machine Intelligence Research Institute.</p>

        <p>This redirected me to the Machine Intelligence Research Institute website (<a href = "https://intelligence.org">https://intelligence.org</a>). The contents on this website are quiet interesting, especially the Blog. Also, they have put their papers online, which lay out the theoretical foundation of building aligned artificial intelligence systems.</p>

        <p>Also, through the website mentioned above, i found out another website <a href = "http://lesswrong.com">lesswrong.com</a>.</p>

        <p>This website in a single word, is amazing. The goal of this website being to explore the rationality of the human brain, and relate it in terms of artificial intelligence. The blogs on this website contain a lot of information and insights into a lot of topics related to human brain and artificial intelligence. It is worth bookmarking.</p>

      <h3>#7.Probability</h3>
        <p>I completed 2 chapters from the book Neural Networks and Learning Machines by Simon Haykin. My realisation was that there was a lot mathematical background required, which I am not familiar with. Hence, I have decided to learn Probability and Linear Algebra, before continuing on to read this book.</p>

        <p>Also, is turns out that MIT’s machine learning course 6.867  (which i briefly talked about in a previous commit that I was going to also complete) requires the following prerequisites :
        <br><a href = "http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/">MIT’s Probability course 6.041</a>
        <br><a href = "http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">MIT’s linear algebra course 18.06</a></p>

        <p>hence, I am now starting with MIT’s <a href = "probability.html">probability</a> course 6.041 in a new, taking a little detour from the Neural Network subject matter.</p>

      </section>
    </div>

    
  </body>
</html>
